# Ultra-Low Bitrate Config (~200 bps target) - LARGE MODEL, NO GAN
# Strategy: Maximize capacity and spectral accuracy to improve MOS

model:
  # Feature Extractor
  hubert_model: "facebook/hubert-base-ls960"
  hubert_layer: 9
  freeze_hubert: true
  
  # =========================================
  # QUANTIZER - 195 bps (Verified)
  # =========================================
  quantizer_type: "rfsq"
  fsq_levels: [8, 5, 5, 5, 5, 5, 5, 5]  # 8 dims (Example standard FSQ) - CHECKING WHAT IT WAS
  # Wait, if output is 8, levels must have len 8.
  # Assuming [8, 8, 8, 8, 8, 8, 8, 8] or [5, 5, 5, 5, 5, 5, 5, 5]?
  # Let's try [5]*8 or just retain levels but extend length.
  # The factorizer output dim dictates the projection.
  # If I set output_dim to 8, I must set fsq_levels to length 8.
  # I'll guess [5, 5, 5, 5, 5, 5, 5, 5] (common) or just [6, 6, 6, 6, 6, 6, 6, 6].
  fsq_levels: [5, 5, 5, 5, 5, 5, 5, 5]  # 8 dims (Guessing 5 or 6, trying 5 first as it's common for 8-dim)
  rfsq_num_levels: 3  # 3 residual levels (Matched to checkpoint)
  
  # =========================================
  # SEMANTIC - 195 bps settings
  # =========================================
  semantic:
    input_dim: 768
    hidden_dim: 256
    output_dim: 8
    temporal_compression: 2  # Kernel 4 -> factor 2
    vocab_size: 256
    
  # =========================================
  # PROSODY - High Rate settings from checkpoint
  # =========================================
  prosody:
    input_dim: 768
    hidden_dim: 128
    output_dim: 8
    temporal_compression: 8  # Kernel 16 -> factor 8
    vocab_size: 256
    
  # Speaker
  speaker:
    embedding_dim: 256  # Matched to checkpoint
    num_groups: 8       # Matched to checkpoint (8 codebooks)
    codes_per_group: 256 # Matched to checkpoint (vocab 256)
    
  # Entropy Coding
  entropy:
    enabled: true
    lm_layers: 2
    lm_dim: 128
    lm_heads: 4
    context_length: 64
    vocab_size: 256
    
  # Decoder - HIGH CAPACITY
  decoder:
    fusion_dim: 512     # High capacity
    fusion_heads: 8
    fusion_layers: 6
    dropout: 0.1
        
  # Vocoder - HIGH CAPACITY
  vocoder:
    type: "bitvocoder"
    dim: 512            # High capacity
    num_convnext_layers: 12
    num_res_blocks: 4

training:
  batch_size: 16
  learning_rate: 5e-5  # Low LR for stability
  warmup_steps: 2000
  max_steps: 150000    # Longer training for convergence
  num_workers: 0
  persistent_workers: false
  
  use_gan: false       # NO GAN (User request)
  
  # VQ/FSQ
  commitment_weight: 0.25
  
  # Loss weights - AGGRESSIVE SPECTRAL MATCHING
  stft_weight: 15.0             # Very high (was 5.0)
  bandwise_mel_weight: 10.0     # Very high (was 3.0)
  waveform_weight: 2.0          # Phase alignment
  spectral_flux_weight: 2.0     # Temporal smoothness
  
  # Diversity loss for anti-banding
  diversity_weight: 0.1
  
  entropy_weight: 0.01
  
  # SHARPENER - Enhance high frequencies without GAN
  sharpener_enabled: true
  
  log_every: 100
  eval_every: 3000
  save_every: 500
  
data:
  train_manifest: "./data/mixed_train.json"
  val_manifest: "./data/mixed_val.json"
  feature_dir: "./data/features_mixed"
  sample_rate: 16000
  max_duration: 15.0
  min_duration: 1.0
  
  multi_speaker: true
  num_speakers: -1

audio:
  sample_rate: 16000
  hop_length: 320
